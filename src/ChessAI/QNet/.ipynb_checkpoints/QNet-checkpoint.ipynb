{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Network implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 512\n",
    "BUFFER_SIZE = 100000\n",
    "GAMMA = 0.999\n",
    "LR = 1e-5\n",
    "SYNC_INTERVAL = 200\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.1\n",
    "EPSILON_DECAY = 0.99999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the board for Q-Net input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_board(board):\n",
    "    # 3D piece encoding (8x8x14)\n",
    "    piece_channels = np.zeros((8, 8, 14), dtype=np.float32)\n",
    "    \n",
    "    for square in chess.SQUARES:\n",
    "        row = 7 - (square // 8)\n",
    "        col = square % 8\n",
    "        piece = board.piece_at(square)\n",
    "        if piece:\n",
    "            channel = piece.piece_type - 1 + (6 * (not piece.color))\n",
    "            piece_channels[row, col, channel] = 1\n",
    "\n",
    "    # Additional features\n",
    "    turn = float(board.turn)\n",
    "    castling = np.array([\n",
    "        board.has_kingside_castling_rights(chess.WHITE),\n",
    "        board.has_queenside_castling_rights(chess.WHITE),\n",
    "        board.has_kingside_castling_rights(chess.BLACK),\n",
    "        board.has_queenside_castling_rights(chess.BLACK)\n",
    "    ], dtype=np.float32)\n",
    "    \n",
    "    check = float(board.is_check())\n",
    "    move_count = np.array([board.fullmove_number / 100], dtype=np.float32)\n",
    "    \n",
    "    # Flatten and concatenate\n",
    "    encoded = np.concatenate([\n",
    "        piece_channels.flatten(),\n",
    "        castling,\n",
    "        [turn],\n",
    "        [check],\n",
    "        move_count\n",
    "    ])\n",
    "    assert len(encoded) == 8*8*14 + 4 + 1 + 1 + 1  # 896 + 7 = 903\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessQNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(14, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, 3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc_input_dim = 512 * 8 * 8 + 9  # 512 channels * 8x8 board + 7 board features + 2 action features\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.fc_input_dim, 1024),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Tách phần board state (8*8*14=896) và các features khác + action (7+2=9)\n",
    "        board_data = x[:, :896].view(-1, 14, 8, 8)\n",
    "        other_features = x[:, 896:896+9]\n",
    "        \n",
    "        # Xử lý qua các lớp convolution\n",
    "        conv_out = self.conv(board_data)\n",
    "        conv_out = conv_out.view(-1, 512 * 8 * 8)\n",
    "        \n",
    "        # Ghép với các features phụ\n",
    "        combined = torch.cat([conv_out, other_features], dim=1)\n",
    "        \n",
    "        return self.fc(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.priorities = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "        self.alpha = 0.6\n",
    "        self.beta = 0.4\n",
    "        self.max_priority = 1.0  # Thêm giá trị khởi tạo\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        # Thêm priority mặc định khi khởi tạo\n",
    "        self.priorities.append(self.max_priority ** self.alpha)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # Xử lý trường hợp tổng priorities = 0\n",
    "        priorities_array = np.array(self.priorities, dtype=np.float32)\n",
    "        total = priorities_array.sum() + 1e-8  # Thêm epsilon để tránh chia 0\n",
    "        \n",
    "        # Chuẩn hóa lại probabilities\n",
    "        probs = priorities_array / total\n",
    "        probs /= probs.sum()  # Đảm bảo tổng bằng 1\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[i] for i in indices]\n",
    "        \n",
    "        # Tính importance sampling weights\n",
    "        weights = (len(self.buffer) * probs[indices]) ** (-self.beta)\n",
    "        weights /= weights.max() + 1e-8  # Thêm epsilon\n",
    "        \n",
    "        return indices, samples, weights\n",
    "\n",
    "    def update_priorities(self, indices, priorities):\n",
    "        # Thêm clipping để tránh giá trị không hợp lệ\n",
    "        priorities = np.clip(priorities, 1e-5, None)\n",
    "        \n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            self.priorities[idx] = (priority + 1e-5) ** self.alpha\n",
    "            self.max_priority = max(self.max_priority, priority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(state, move):\n",
    "    # Random rotation (0-3) and flip\n",
    "    rotation = random.randint(0, 3)\n",
    "    flip = random.choice([True, False])\n",
    "    \n",
    "    # Rotate board state\n",
    "    piece_data = state[:8*8*14].reshape(8, 8, 14)\n",
    "    piece_data = np.rot90(piece_data, rotation)\n",
    "    if flip:\n",
    "        piece_data = np.fliplr(piece_data)\n",
    "    rotated_state = np.concatenate([piece_data.flatten(), state[8*8*14:]])\n",
    "    \n",
    "    # Rotate move coordinates\n",
    "    from_sq = move.from_square\n",
    "    to_sq = move.to_square\n",
    "    \n",
    "    for _ in range(rotation):\n",
    "        from_sq = chess.square_mirror(from_sq)\n",
    "        to_sq = chess.square_mirror(to_sq)\n",
    "    \n",
    "    if flip:\n",
    "        from_sq = chess.square_mirror(from_sq)\n",
    "        to_sq = chess.square_mirror(to_sq)\n",
    "    \n",
    "    return rotated_state, chess.Move(from_sq, to_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_move(board, model):\n",
    "    legal_moves = list(board.legal_moves)\n",
    "    if not legal_moves:\n",
    "        return None\n",
    "\n",
    "    state = encode_board(board)  # 903 features\n",
    "    action_features = np.array(\n",
    "        [[m.from_square / 63, m.to_square / 63] for m in legal_moves]\n",
    "    )\n",
    "\n",
    "    # Tạo input đúng cấu trúc: 903 board features + 2 action features\n",
    "    network_input = np.concatenate(\n",
    "        [np.tile(state, (len(legal_moves), 1)), action_features], axis=1\n",
    "    )\n",
    "\n",
    "    input_tensor = torch.FloatTensor(network_input).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        q_values = model(input_tensor).cpu().numpy().flatten()\n",
    "\n",
    "    return legal_moves[np.argmax(q_values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import asyncio\n",
    "import chess.engine\n",
    "def evaluate_model(model, num_games=10):\n",
    "    win_rate = 0\n",
    "    stockfish = None  # Khởi tạo trước để tránh UnboundLocalError\n",
    "    \n",
    "    try:\n",
    "        # Chỉ định đường dẫn chính xác đến Stockfish\n",
    "        stockfish_path = \"./stockfish/stockfish-windows-x86-64-avx2.exe\"\n",
    "        stockfish = chess.engine.SimpleEngine.popen_uci(stockfish_path)\n",
    "        \n",
    "        # Fix event loop cho Windows\n",
    "        if sys.platform == \"win32\":\n",
    "            policy = asyncio.WindowsProactorEventLoopPolicy()\n",
    "            asyncio.set_event_loop_policy(policy)\n",
    "            \n",
    "        for _ in range(num_games):\n",
    "            board = chess.Board()\n",
    "            while not board.is_game_over():\n",
    "                if board.turn == chess.WHITE:\n",
    "                    move = get_best_move(board, model)\n",
    "                else:\n",
    "                    result = stockfish.play(board, chess.engine.Limit(time=0.1))\n",
    "                    move = result.move\n",
    "                board.push(move)\n",
    "                \n",
    "            if board.result() == \"1-0\":\n",
    "                win_rate += 1\n",
    "                \n",
    "        return win_rate / num_games\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đánh giá: {e}\")\n",
    "        return 0.0\n",
    "    finally:\n",
    "        if stockfish is not None:  # Chỉ gọi quit() nếu đã khởi tạo thành công\n",
    "            stockfish.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reward(board, move):\n",
    "    reward = 0\n",
    "    \n",
    "    # Tạo bản copy của board để không làm thay đổi board gốc\n",
    "    temp_board = board.copy()\n",
    "    \n",
    "    # 1. Giá trị quân cờ (material balance)\n",
    "    piece_values = {\n",
    "        chess.PAWN: 1,\n",
    "        chess.KNIGHT: 3,\n",
    "        chess.BISHOP: 3.2,\n",
    "        chess.ROOK: 5,\n",
    "        chess.QUEEN: 9,\n",
    "        chess.KING: 0\n",
    "    }\n",
    "    \n",
    "    # Tính toán material trước và sau nước đi\n",
    "    material_before = sum(piece_values[p.piece_type] for p in temp_board.piece_map().values())\n",
    "    \n",
    "    # Thực hiện move trên bản copy\n",
    "    if temp_board.is_legal(move):  # Kiểm tra tính hợp lệ trước khi push\n",
    "        temp_board.push(move)\n",
    "        material_after = sum(piece_values[p.piece_type] for p in temp_board.piece_map().values())\n",
    "    else:\n",
    "        # Phạt nặng nếu move không hợp lệ\n",
    "        return -10.0\n",
    "    \n",
    "    reward += (material_after - material_before) * 0.1\n",
    "\n",
    "    # 2. Kiểm soát trung tâm\n",
    "    center_squares = [chess.D4, chess.D5, chess.E4, chess.E5]\n",
    "    center_control = sum(1 for sq in center_squares if temp_board.is_attacked_by(temp_board.turn, sq))\n",
    "    reward += center_control * 0.05\n",
    "\n",
    "    # 3. An toàn của vua\n",
    "    king_square = temp_board.king(temp_board.turn)\n",
    "    safety_penalty = -0.02 * len(temp_board.attackers(not temp_board.turn, king_square))\n",
    "    reward += safety_penalty\n",
    "\n",
    "    # 4. Hoạt động của quân\n",
    "    mobility = len(list(temp_board.legal_moves)) / 100\n",
    "    reward += mobility * 0.1\n",
    "\n",
    "    # 5. Phạt đứng yên\n",
    "    if temp_board.is_repetition(2):\n",
    "        reward -= 0.1\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_self_play_games(model, num_games=10):\n",
    "    buffer = []\n",
    "    for _ in range(num_games):\n",
    "        board = chess.Board()\n",
    "        game_history = []\n",
    "\n",
    "        while not board.is_game_over():\n",
    "            # Epsilon-greedy exploration\n",
    "            if random.random() < EPSILON_START:\n",
    "                move = random.choice(list(board.legal_moves))\n",
    "            else:\n",
    "                move = get_best_move(board, model)\n",
    "\n",
    "            prev_fen = board.fen()\n",
    "            board.push(move)\n",
    "            next_fen = board.fen()\n",
    "\n",
    "            game_history.append(\n",
    "                (\n",
    "                    encode_board(chess.Board(prev_fen)),\n",
    "                    move,\n",
    "                    calculate_reward(board, move),\n",
    "                    next_fen,  # Lưu FEN thay vì encoded state\n",
    "                    board.is_game_over(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Gán thêm reward cuối trận\n",
    "        result = board.result()\n",
    "        final_reward = 1.0 if result == \"1-0\" else -1.0 if result == \"0-1\" else 0\n",
    "        for i, (s, m, r, ns, d) in enumerate(game_history):\n",
    "            game_history[i] = (\n",
    "                s,\n",
    "                m,\n",
    "                r + final_reward,\n",
    "                ns,\n",
    "                d,\n",
    "            )  # Cộng thêm reward cuối trận\n",
    "\n",
    "        buffer.extend(game_history)\n",
    "    return buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Lỗi khi đánh giá: [Errno 2] No such file or directory: './stockfish/stockfish-windows-x86-64-avx2.exe'\n",
      "Episode 0: Win Rate 0.00, Epsilon 1.00\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 125\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 125\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 74\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m next_actions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m board \u001b[38;5;129;01min\u001b[39;00m next_boards:\n\u001b[0;32m---> 74\u001b[0m     move \u001b[38;5;241m=\u001b[39m \u001b[43mget_best_move\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_net\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m move \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         next_actions\u001b[38;5;241m.\u001b[39mappend(move)\n",
      "Cell \u001b[0;32mIn[10], line 19\u001b[0m, in \u001b[0;36mget_best_move\u001b[0;34m(board, model)\u001b[0m\n\u001b[1;32m     16\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(network_input)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 19\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m legal_moves[np\u001b[38;5;241m.\u001b[39margmax(q_values)]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    # Initialize networks\n",
    "    q_net = ChessQNetwork().to(DEVICE)\n",
    "    target_net = ChessQNetwork().to(DEVICE)\n",
    "    target_net.load_state_dict(q_net.state_dict())\n",
    "    optimizer = optim.AdamW(q_net.parameters(), lr=LR)\n",
    "\n",
    "    # Initialize buffer and logger\n",
    "    replay_buffer = PrioritizedReplayBuffer(BUFFER_SIZE)\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    epsilon = EPSILON_START\n",
    "    step_counter = 0\n",
    "\n",
    "    for episode in range(10000):\n",
    "        sample_input = torch.randn(1, 896 + 9).to(DEVICE)  # Đúng kích thước đầu vào\n",
    "        print(\"Input shape:\", sample_input.shape)\n",
    "        output = q_net(sample_input)\n",
    "        print(\"Output shape:\", output.shape)\n",
    "        # Generate self-play games\n",
    "        games = generate_self_play_games(q_net)\n",
    "        for game in games:\n",
    "            replay_buffer.add(game)\n",
    "\n",
    "        # Training step\n",
    "        if len(replay_buffer.buffer) >= BATCH_SIZE:\n",
    "            indices, batch, weights = replay_buffer.sample(BATCH_SIZE)\n",
    "\n",
    "            # Unpack batch\n",
    "            states, moves, rewards, next_fens, dones = zip(*batch)\n",
    "\n",
    "            # Convert to tensors\n",
    "            state_tensor = torch.FloatTensor(np.array(states)).to(DEVICE)\n",
    "            action_tensor = torch.FloatTensor(\n",
    "                [[m.from_square / 63, m.to_square / 63] for m in moves]\n",
    "            ).to(DEVICE)\n",
    "            reward_tensor = torch.FloatTensor(rewards).to(DEVICE)\n",
    "            done_tensor = torch.BoolTensor(dones).to(DEVICE)\n",
    "            weights_tensor = torch.FloatTensor(weights).to(DEVICE)\n",
    "\n",
    "            # Mã hóa next states từ FEN\n",
    "            next_state_tensor = torch.stack(\n",
    "                [torch.FloatTensor(encode_board(chess.Board(fen))) for fen in next_fens]\n",
    "            ).to(DEVICE)\n",
    "            action_tensor = torch.FloatTensor(\n",
    "                [[m.from_square / 63, m.to_square / 63] for m in moves]\n",
    "            ).to(DEVICE)\n",
    "            reward_tensor = torch.FloatTensor(rewards).to(DEVICE)\n",
    "            done_tensor = torch.BoolTensor(dones).to(DEVICE)\n",
    "            weights_tensor = torch.FloatTensor(weights).to(DEVICE)\n",
    "\n",
    "            # Compute Q values\n",
    "            network_input = torch.cat([state_tensor, action_tensor], dim=1)\n",
    "            current_q = q_net(network_input).squeeze()\n",
    "\n",
    "            # Compute target Q values\n",
    "            with torch.no_grad():\n",
    "                next_q = torch.zeros(BATCH_SIZE, device=DEVICE)\n",
    "                valid_next = ~done_tensor\n",
    "\n",
    "                if any(valid_next):\n",
    "                    # Chuyển valid_next sang numpy array trên CPU\n",
    "                    valid_next_indices = np.where(valid_next.cpu().numpy())[0]\n",
    "\n",
    "                    # Lấy các FEN tương ứng\n",
    "                    selected_next_fens = [next_fens[i] for i in valid_next_indices]\n",
    "\n",
    "                    # Tạo các bàn cờ từ FEN\n",
    "                    next_boards = [chess.Board(fen) for fen in selected_next_fens]\n",
    "\n",
    "                    # Lấy các nước đi tốt nhất\n",
    "                    next_actions = []\n",
    "                    for board in next_boards:\n",
    "                        move = get_best_move(board, target_net)\n",
    "                        if move is not None:\n",
    "                            next_actions.append(move)\n",
    "\n",
    "                    # Tạo tensor đầu vào\n",
    "                    next_action_tensor = torch.FloatTensor(\n",
    "                        [[m.from_square / 63, m.to_square / 63] for m in next_actions]\n",
    "                    ).to(DEVICE)\n",
    "\n",
    "                    # Chuẩn bị dữ liệu đầu vào\n",
    "                    next_inputs = torch.cat(\n",
    "                        [next_state_tensor[valid_next], next_action_tensor], dim=1\n",
    "                    )\n",
    "                    next_q[valid_next] = target_net(next_inputs).squeeze()\n",
    "\n",
    "                target_q = reward_tensor + GAMMA * next_q\n",
    "\n",
    "            # Compute loss\n",
    "            loss = (weights_tensor * (current_q - target_q).pow(2)).mean()\n",
    "\n",
    "            # Optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(q_net.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update priorities\n",
    "            new_priorities = (current_q - target_q).abs().detach().cpu().numpy() + 1e-5\n",
    "            replay_buffer.update_priorities(indices, new_priorities)\n",
    "\n",
    "            # Logging\n",
    "            writer.add_scalar(\"Loss\", loss.item(), step_counter)\n",
    "            step_counter += 1\n",
    "\n",
    "        # Sync target network\n",
    "        if episode % SYNC_INTERVAL == 0:\n",
    "            target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "        # Decay epsilon\n",
    "        epsilon = max(EPSILON_END, epsilon * EPSILON_DECAY)\n",
    "\n",
    "        # Evaluation\n",
    "        if episode % 10 == 0:\n",
    "            win_rate = evaluate_model(q_net)\n",
    "            writer.add_scalar(\"Win Rate\", torch.tensor(win_rate), episode)\n",
    "            print(f\"Episode {episode}: Win Rate {win_rate:.2f}, Epsilon {epsilon:.2f}\")\n",
    "    torch.save(q_net.state_dict(), \"chess_ai.pth\")\n",
    "    print(\"Model saved successfully!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
