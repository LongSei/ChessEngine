{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11b206be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import chess\n",
    "import chess.engine\n",
    "import chess.svg\n",
    "import random\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from models.MiniDeepchess import SiameseNetwork, AutoEncoder\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a82681",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a76e7494",
   "metadata": {},
   "outputs": [],
   "source": [
    "whiteWonFile = \"./data/whiteWin.npy\"\n",
    "whiteLostFile = \"./data/blackWin.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc8a589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Loading\n",
    "whiteWonStates = np.load(whiteWonFile)\n",
    "whiteLostStates = np.load(whiteLostFile)\n",
    "\n",
    "# Custom Dataset Class\n",
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, whiteWonStates, whiteLostStates):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with white won and white lost states.\n",
    "        \n",
    "        Args:\n",
    "            whiteWonStates (np.ndarray): Array of positions where white wins.\n",
    "            whiteLostStates (np.ndarray): Array of positions where white loses.\n",
    "        \"\"\"\n",
    "        self.sampleSize = min(len(whiteWonStates), len(whiteLostStates))\n",
    "        # Shuffle the states initially\n",
    "        self.whiteWonStates = whiteWonStates.copy()\n",
    "        self.whiteLostStates = whiteLostStates.copy()\n",
    "        np.random.shuffle(self.whiteWonStates)\n",
    "        np.random.shuffle(self.whiteLostStates)\n",
    "        # Take only the first sampleSize samples\n",
    "        self.whiteWonStates = self.whiteWonStates[:self.sampleSize]\n",
    "        self.whiteLostStates = self.whiteLostStates[:self.sampleSize]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of samples in the dataset.\"\"\"\n",
    "        return self.sampleSize\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get a sample pair and its labels.\n",
    "        \n",
    "        Args:\n",
    "            index (int): Index of the sample.\n",
    "            \n",
    "        Returns:\n",
    "            tuple: ([X1, X2], Y) where X1 and X2 are position tensors, and Y is the label tensor.\n",
    "        \"\"\"\n",
    "        X1 = self.whiteWonStates[index]\n",
    "        X2 = self.whiteLostStates[index]\n",
    "        \n",
    "        # Randomly swap with 50% probability\n",
    "        if random.random() < 0.5:\n",
    "            X1, X2 = X2, X1\n",
    "            Y1, Y2 = 1, 0  # 1 for white lost, 0 for white won\n",
    "        else:\n",
    "            Y1, Y2 = 0, 1  # 0 for white won, 1 for white lost\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X1 = torch.from_numpy(X1).float()\n",
    "        X2 = torch.from_numpy(X2).float()\n",
    "        Y = torch.tensor([Y1, Y2], dtype=torch.float32)\n",
    "        \n",
    "        return [X1, X2], Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2522222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1800000 white wins, 1800000 black wins\n",
      "Test: 200000 white wins, 200000 black wins\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(whiteWinTrain)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m white wins, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(whiteLostTrain)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m black wins\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTest: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(whiteWinTest)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m white wins, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(whiteLostTest)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m black wins\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m dataset_train = \u001b[43mSiameseDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhiteWonStates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhiteWinTrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhiteLostStates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhiteLostTrain\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m dataloader_train = DataLoader(dataset_train, batch_size=\u001b[32m64\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     20\u001b[39m dataset_test = SiameseDataset(\n\u001b[32m     21\u001b[39m     whiteWonStates=whiteWinTest,\n\u001b[32m     22\u001b[39m     whiteLostStates=whiteLostTest\n\u001b[32m     23\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mSiameseDataset.__init__\u001b[39m\u001b[34m(self, whiteWonStates, whiteLostStates)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mself\u001b[39m.sampleSize = \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(whiteWonStates), \u001b[38;5;28mlen\u001b[39m(whiteLostStates))\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Shuffle the states initially\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28mself\u001b[39m.whiteWonStates = \u001b[43mwhiteWonStates\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mself\u001b[39m.whiteLostStates = whiteLostStates.copy()\n\u001b[32m     19\u001b[39m np.random.shuffle(\u001b[38;5;28mself\u001b[39m.whiteWonStates)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "percentTrain = 0.9\n",
    "\n",
    "whiteWin = np.load(\"./data/whiteWin.npy\")\n",
    "whiteLost = np.load(\"./data/blackWin.npy\")\n",
    "\n",
    "whiteWinTrain = whiteWin[:int(len(whiteWin) * percentTrain)]\n",
    "whiteLostTrain = whiteLost[:int(len(whiteLost) * percentTrain)]\n",
    "whiteWinTest = whiteWin[int(len(whiteWin) * percentTrain):]\n",
    "whiteLostTest = whiteLost[int(len(whiteLost) * percentTrain):]\n",
    "\n",
    "print(f\"Train: {len(whiteWinTrain)} white wins, {len(whiteLostTrain)} black wins\")\n",
    "print(f\"Test: {len(whiteWinTest)} white wins, {len(whiteLostTest)} black wins\")\n",
    "\n",
    "dataset_train = SiameseDataset(\n",
    "    whiteWonStates=whiteWinTrain,\n",
    "    whiteLostStates=whiteLostTrain\n",
    ")\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=64, shuffle=True)\n",
    "\n",
    "dataset_test = SiameseDataset(\n",
    "    whiteWonStates=whiteWinTest,\n",
    "    whiteLostStates=whiteLostTest\n",
    ")\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2163a4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded comparator from ./checkpoints/Siamese.pth\n",
      "Loaded feature extractor from ./checkpoints/AutoEncoder.pth\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SiameseNetwork(\n",
    "    feature_extractor=AutoEncoder, \n",
    "    feature_extractor_layers=[773, 600, 400, 200, 100], \n",
    "    comparator_layers=[400, 200, 200, 100, 100, 30, 30],\n",
    "    output_dim=2\n",
    ").to(device)\n",
    "model.load_pretrained(\n",
    "    feature_extractor_path=\"./checkpoints/AutoEncoder.pth\",\n",
    "    comparator_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8fee19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SiameseNetwork(\n",
      "  (feature_extractor): AutoEncoder(\n",
      "    (encoder): Sequential(\n",
      "      (0): Linear(in_features=773, out_features=600, bias=True)\n",
      "      (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "      (3): Linear(in_features=600, out_features=400, bias=True)\n",
      "      (4): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): LeakyReLU(negative_slope=0.01)\n",
      "      (6): Linear(in_features=400, out_features=200, bias=True)\n",
      "      (7): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): LeakyReLU(negative_slope=0.01)\n",
      "      (9): Linear(in_features=200, out_features=100, bias=True)\n",
      "      (10): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (11): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (decoder): Sequential(\n",
      "      (0): Linear(in_features=100, out_features=200, bias=True)\n",
      "      (1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "      (3): Linear(in_features=200, out_features=400, bias=True)\n",
      "      (4): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): LeakyReLU(negative_slope=0.01)\n",
      "      (6): Linear(in_features=400, out_features=600, bias=True)\n",
      "      (7): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): LeakyReLU(negative_slope=0.01)\n",
      "      (9): Linear(in_features=600, out_features=773, bias=True)\n",
      "      (10): BatchNorm1d(773, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (11): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      "  (comparator): Sequential(\n",
      "    (0): Linear(in_features=200, out_features=400, bias=True)\n",
      "    (1): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Linear(in_features=400, out_features=200, bias=True)\n",
      "    (4): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=200, out_features=200, bias=True)\n",
      "    (7): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): LeakyReLU(negative_slope=0.01)\n",
      "    (9): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (10): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (13): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): LeakyReLU(negative_slope=0.01)\n",
      "    (15): Linear(in_features=100, out_features=30, bias=True)\n",
      "    (16): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (17): LeakyReLU(negative_slope=0.01)\n",
      "    (18): Linear(in_features=30, out_features=30, bias=True)\n",
      "    (19): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (20): LeakyReLU(negative_slope=0.01)\n",
      "    (21): Linear(in_features=30, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967b29de",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = F.binary_cross_entropy\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf6776f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        (X1, X2), Y = batch\n",
    "        X1, X2, Y = X1.to(device), X2.to(device), Y.to(device)\n",
    "        \n",
    "        output = model(X1, X2)\n",
    "        loss = criterion(output, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Step the scheduler at the end of each epoch\n",
    "    scheduler.step()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for (X1, X2), Y in tqdm(dataloader, desc=\"Testing\", leave=False):\n",
    "        X1, X2, Y = X1.to(device), X2.to(device), Y.to(device)\n",
    "        output = model(X1, X2)\n",
    "        loss = criterion(output, Y)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Convert probabilities to predicted classes\n",
    "        pred = torch.argmax(output, dim=1)\n",
    "        target = torch.argmax(Y, dim=1)\n",
    "\n",
    "        correct = (pred == target).sum().item()\n",
    "        total_correct += correct\n",
    "        total_samples += Y.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0795453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Train Loss: 0.6354, Test Loss: 1.0111, Accuracy: 0.6285\n",
      "Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000, Train Loss: 0.6322, Test Loss: 0.7291, Accuracy: 0.6298\n",
      "Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000, Train Loss: 0.6299, Test Loss: 1.0153, Accuracy: 0.6297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000, Train Loss: 0.6277, Test Loss: 1.0825, Accuracy: 0.6297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000, Train Loss: 0.6261, Test Loss: 1.1657, Accuracy: 0.6312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000, Train Loss: 0.6245, Test Loss: 1.2694, Accuracy: 0.6288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000, Train Loss: 0.6230, Test Loss: 0.7832, Accuracy: 0.6352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000, Train Loss: 0.6216, Test Loss: 1.1499, Accuracy: 0.6325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000, Train Loss: 0.6204, Test Loss: 0.8594, Accuracy: 0.6339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000, Train Loss: 0.6195, Test Loss: 0.8576, Accuracy: 0.6336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000, Train Loss: 0.6131, Test Loss: 1.2221, Accuracy: 0.6324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000, Train Loss: 0.6113, Test Loss: 0.8395, Accuracy: 0.6364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000, Train Loss: 0.6103, Test Loss: 0.8293, Accuracy: 0.6352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/1000, Train Loss: 0.6098, Test Loss: 0.9317, Accuracy: 0.6353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/1000, Train Loss: 0.6091, Test Loss: 1.2236, Accuracy: 0.6336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/1000, Train Loss: 0.6089, Test Loss: 1.0399, Accuracy: 0.6344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/1000, Train Loss: 0.6082, Test Loss: 0.9326, Accuracy: 0.6324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/1000, Train Loss: 0.6080, Test Loss: 0.8369, Accuracy: 0.6350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/1000, Train Loss: 0.6076, Test Loss: 0.8194, Accuracy: 0.6343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/1000, Train Loss: 0.6072, Test Loss: 0.8927, Accuracy: 0.6338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/1000, Train Loss: 0.6062, Test Loss: 0.9076, Accuracy: 0.6347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/1000, Train Loss: 0.6063, Test Loss: 0.8617, Accuracy: 0.6353\n",
      "Early stopping!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "best_loss = float(\"inf\")\n",
    "patience = 20\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, dataloader_train, optimizer, criterion, scheduler, device)\n",
    "    test_loss, acc = test(model, dataloader_test, criterion, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Accuracy: {acc:.4f}\")\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        patience = 20\n",
    "        torch.save(model.feature_extractor.state_dict(), \"checkpoints/AutoEncoder.pth\")\n",
    "        torch.save(model.comparator.state_dict(), \"checkpoints/Siamese.pth\")\n",
    "        print(\"Model saved!\")\n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience == 0:\n",
    "            print(\"Early stopping!\")\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ChessEngine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
