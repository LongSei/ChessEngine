{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Network implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 100000\n",
    "GAMMA = 0.99\n",
    "LR = 3e-4\n",
    "SYNC_INTERVAL = 50\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.1\n",
    "EPSILON_DECAY = 0.9995"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the board for Q-Net input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_board(board):\n",
    "    # 3D piece encoding (8x8x14)\n",
    "    piece_channels = np.zeros((8, 8, 14), dtype=np.float32)\n",
    "    \n",
    "    for square in chess.SQUARES:\n",
    "        row = 7 - (square // 8)\n",
    "        col = square % 8\n",
    "        piece = board.piece_at(square)\n",
    "        if piece:\n",
    "            channel = piece.piece_type - 1 + (6 * (not piece.color))\n",
    "            piece_channels[row, col, channel] = 1\n",
    "\n",
    "    # Additional features\n",
    "    turn = float(board.turn)\n",
    "    castling = np.array([\n",
    "        board.has_kingside_castling_rights(chess.WHITE),\n",
    "        board.has_queenside_castling_rights(chess.WHITE),\n",
    "        board.has_kingside_castling_rights(chess.BLACK),\n",
    "        board.has_queenside_castling_rights(chess.BLACK)\n",
    "    ], dtype=np.float32)\n",
    "    \n",
    "    check = float(board.is_check())\n",
    "    move_count = np.array([board.fullmove_number / 100], dtype=np.float32)\n",
    "    \n",
    "    # Flatten and concatenate\n",
    "    encoded = np.concatenate([\n",
    "        piece_channels.flatten(),\n",
    "        castling,\n",
    "        [turn],\n",
    "        [check],\n",
    "        move_count\n",
    "    ])\n",
    "    \n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessQNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # CNN for board processing (giữ nguyên)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(14, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Sửa input dimension của fc layer\n",
    "        self.fc_input_dim = 256 * 8 * 8 + 7 + 2  # Thêm 2 chiều cho action\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.fc_input_dim, 512),  # Đã cập nhật input dim\n",
    "            nn.Dropout(0.3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Tách board state và action features\n",
    "        board_data = x[:, :8*8*14].view(-1, 14, 8, 8)\n",
    "        other_features = x[:, 8*8*14:]\n",
    "        \n",
    "        # Xử lý board\n",
    "        conv_out = self.conv(board_data).view(-1, 256*8*8)\n",
    "        \n",
    "        # Ghép với các features khác và action\n",
    "        combined = torch.cat([conv_out, other_features], dim=1)\n",
    "        \n",
    "        return self.fc(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.priorities = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "        self.alpha = 0.6\n",
    "        self.beta = 0.4\n",
    "        self.max_priority = 1.0  # Thêm giá trị khởi tạo\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        # Thêm priority mặc định khi khởi tạo\n",
    "        self.priorities.append(self.max_priority ** self.alpha)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # Xử lý trường hợp tổng priorities = 0\n",
    "        priorities_array = np.array(self.priorities, dtype=np.float32)\n",
    "        total = priorities_array.sum() + 1e-8  # Thêm epsilon để tránh chia 0\n",
    "        \n",
    "        # Chuẩn hóa lại probabilities\n",
    "        probs = priorities_array / total\n",
    "        probs /= probs.sum()  # Đảm bảo tổng bằng 1\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[i] for i in indices]\n",
    "        \n",
    "        # Tính importance sampling weights\n",
    "        weights = (len(self.buffer) * probs[indices]) ** (-self.beta)\n",
    "        weights /= weights.max() + 1e-8  # Thêm epsilon\n",
    "        \n",
    "        return indices, samples, weights\n",
    "\n",
    "    def update_priorities(self, indices, priorities):\n",
    "        # Thêm clipping để tránh giá trị không hợp lệ\n",
    "        priorities = np.clip(priorities, 1e-5, None)\n",
    "        \n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            self.priorities[idx] = (priority + 1e-5) ** self.alpha\n",
    "            self.max_priority = max(self.max_priority, priority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_self_play_games(model, num_games=10):\n",
    "    buffer = []\n",
    "    for _ in range(num_games):\n",
    "        board = chess.Board()\n",
    "        game_history = []\n",
    "        \n",
    "        while not board.is_game_over():\n",
    "            # Epsilon-greedy exploration\n",
    "            if random.random() < EPSILON_START:\n",
    "                move = random.choice(list(board.legal_moves))\n",
    "            else:\n",
    "                move = get_best_move(board, model)\n",
    "            \n",
    "            prev_state = encode_board(board)\n",
    "            board.push(move)\n",
    "            next_state = encode_board(board)\n",
    "            \n",
    "            # Store original experience\n",
    "            game_history.append((prev_state, move, next_state))\n",
    "            \n",
    "            # Data augmentation\n",
    "            for _ in range(2):  # Random flips/rotations\n",
    "                rotated_state, rotated_move = augment_data(prev_state, move)\n",
    "                game_history.append((rotated_state, rotated_move, next_state))\n",
    "        \n",
    "        # Assign final rewards\n",
    "        result = board.result()\n",
    "        reward = 1.0 if result == \"1-0\" else -1.0 if result == \"0-1\" else 0\n",
    "        \n",
    "        # Add to buffer with computed rewards\n",
    "        for state, move, next_state in game_history:\n",
    "            buffer.append((state, move, reward, next_state, board.is_game_over()))\n",
    "        \n",
    "    return buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(state, move):\n",
    "    # Random rotation (0-3) and flip\n",
    "    rotation = random.randint(0, 3)\n",
    "    flip = random.choice([True, False])\n",
    "    \n",
    "    # Rotate board state\n",
    "    piece_data = state[:8*8*14].reshape(8, 8, 14)\n",
    "    piece_data = np.rot90(piece_data, rotation)\n",
    "    if flip:\n",
    "        piece_data = np.fliplr(piece_data)\n",
    "    rotated_state = np.concatenate([piece_data.flatten(), state[8*8*14:]])\n",
    "    \n",
    "    # Rotate move coordinates\n",
    "    from_sq = move.from_square\n",
    "    to_sq = move.to_square\n",
    "    \n",
    "    for _ in range(rotation):\n",
    "        from_sq = chess.square_mirror(from_sq)\n",
    "        to_sq = chess.square_mirror(to_sq)\n",
    "    \n",
    "    if flip:\n",
    "        from_sq = chess.square_mirror(from_sq)\n",
    "        to_sq = chess.square_mirror(to_sq)\n",
    "    \n",
    "    return rotated_state, chess.Move(from_sq, to_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_move(board, model):\n",
    "    legal_moves = list(board.legal_moves)\n",
    "    if not legal_moves:\n",
    "        return None\n",
    "    \n",
    "    # Chuẩn bị batch input đúng định dạng\n",
    "    state = encode_board(board)\n",
    "    state_repeated = np.tile(state, (len(legal_moves), 1))\n",
    "    \n",
    "    # Thêm action features\n",
    "    action_features = np.array([[m.from_square/63, m.to_square/63] for m in legal_moves])\n",
    "    network_input = np.concatenate([state_repeated, action_features], axis=1)\n",
    "    \n",
    "    # Chuyển sang tensor\n",
    "    input_tensor = torch.FloatTensor(network_input).to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        q_values = model(input_tensor).cpu().numpy().flatten()\n",
    "    \n",
    "    return legal_moves[np.argmax(q_values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Episode 0: Win Rate 0.00, Epsilon 1.00\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Episode 10: Win Rate 0.00, Epsilon 0.99\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Episode 20: Win Rate 0.00, Epsilon 0.99\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Episode 30: Win Rate 0.10, Epsilon 0.98\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Episode 40: Win Rate 0.00, Epsilon 0.98\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input shape: torch.Size([1, 905])\n",
      "Output shape: torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    # Initialize networks\n",
    "    q_net = ChessQNetwork().to(DEVICE)\n",
    "    target_net = ChessQNetwork().to(DEVICE)\n",
    "    target_net.load_state_dict(q_net.state_dict())\n",
    "    optimizer = optim.AdamW(q_net.parameters(), lr=LR)\n",
    "\n",
    "    # Initialize buffer and logger\n",
    "    replay_buffer = PrioritizedReplayBuffer(BUFFER_SIZE)\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    epsilon = EPSILON_START\n",
    "    step_counter = 0\n",
    "\n",
    "    for episode in range(1000):  # Thêm các dòng kiểm tra này vào đầu training loop\n",
    "        sample_input = torch.randn(1, 8 * 8 * 14 + 7 + 2).to(\n",
    "            DEVICE\n",
    "        )  # 903 (state) + 2 (action) = 905\n",
    "        print(\"Input shape:\", sample_input.shape)\n",
    "        output = q_net(sample_input)\n",
    "        print(\"Output shape:\", output.shape)\n",
    "        # Generate self-play games\n",
    "        games = generate_self_play_games(q_net, num_games=5)\n",
    "        for game in games:\n",
    "            replay_buffer.add(game)\n",
    "\n",
    "        # Training step\n",
    "        if len(replay_buffer.buffer) >= BATCH_SIZE:\n",
    "            indices, batch, weights = replay_buffer.sample(BATCH_SIZE)\n",
    "\n",
    "            # Unpack batch\n",
    "            states, moves, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "            # Convert to tensors\n",
    "            state_tensor = torch.FloatTensor(np.array(states)).to(DEVICE)\n",
    "            action_tensor = torch.FloatTensor(\n",
    "                [[m.from_square / 63, m.to_square / 63] for m in moves]\n",
    "            ).to(DEVICE)\n",
    "            next_state_tensor = torch.FloatTensor(np.array(next_states)).to(DEVICE)\n",
    "            reward_tensor = torch.FloatTensor(rewards).to(DEVICE)\n",
    "            done_tensor = torch.BoolTensor(dones).to(DEVICE)\n",
    "            weights_tensor = torch.FloatTensor(weights).to(DEVICE)\n",
    "\n",
    "            # Compute Q values\n",
    "            network_input = torch.cat([state_tensor, action_tensor], dim=1)\n",
    "            current_q = q_net(network_input).squeeze()\n",
    "\n",
    "            # Compute target Q values\n",
    "            with torch.no_grad():\n",
    "                next_q = torch.zeros(BATCH_SIZE, device=DEVICE)\n",
    "                valid_next = ~done_tensor\n",
    "                if any(valid_next):\n",
    "                    next_actions = [\n",
    "                        get_best_move(\n",
    "                            chess.Board().set_fen(chess.Board().fen()), target_net\n",
    "                        )\n",
    "                        for _ in range(sum(valid_next))\n",
    "                    ]\n",
    "                    next_action_tensor = torch.FloatTensor(\n",
    "                        [[m.from_square / 63, m.to_square / 63] for m in next_actions]\n",
    "                    ).to(DEVICE)\n",
    "                    next_inputs = torch.cat(\n",
    "                        [next_state_tensor[valid_next], next_action_tensor], dim=1\n",
    "                    )\n",
    "                    next_q[valid_next] = target_net(next_inputs).squeeze()\n",
    "\n",
    "                target_q = reward_tensor + GAMMA * next_q\n",
    "\n",
    "            # Compute loss\n",
    "            loss = (weights_tensor * (current_q - target_q).pow(2)).mean()\n",
    "\n",
    "            # Optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(q_net.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update priorities\n",
    "            new_priorities = (current_q - target_q).abs().detach().cpu().numpy() + 1e-5\n",
    "            replay_buffer.update_priorities(indices, new_priorities)\n",
    "\n",
    "            # Logging\n",
    "            writer.add_scalar(\"Loss\", loss.item(), step_counter)\n",
    "            step_counter += 1\n",
    "\n",
    "        # Sync target network\n",
    "        if episode % SYNC_INTERVAL == 0:\n",
    "            target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "        # Decay epsilon\n",
    "        epsilon = max(EPSILON_END, epsilon * EPSILON_DECAY)\n",
    "\n",
    "        # Evaluation\n",
    "        if episode % 10 == 0:\n",
    "            win_rate = evaluate(q_net)\n",
    "            writer.add_scalar(\"Win Rate\", win_rate, episode)\n",
    "            print(f\"Episode {episode}: Win Rate {win_rate:.2f}, Epsilon {epsilon:.2f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chess-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
