{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "393759d3",
   "metadata": {},
   "source": [
    "# Ref\n",
    "https://github.com/shafitek/DeepChess-AI/blob/master/scripts/DeepChess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e52130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import chess\n",
    "import chess.engine\n",
    "import chess.svg\n",
    "import random\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from models.MiniDeepchess import SiameseNetwork, AutoEncoder\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c1c16ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoderChessDataset(Dataset):\n",
    "    def __init__(self, whiteWonStates, whiteLostStates):\n",
    "        \"\"\"\n",
    "        Dataset dÃ¹ng Ä‘á»ƒ huáº¥n luyá»‡n AutoEncoder tá»« cÃ¡c tráº¡ng thÃ¡i tháº¯ng vÃ  thua.\n",
    "\n",
    "        Args:\n",
    "            whiteWonStates (np.ndarray): Tráº¡ng thÃ¡i bÃ n cá» khi tráº¯ng tháº¯ng.\n",
    "            whiteLostStates (np.ndarray): Tráº¡ng thÃ¡i bÃ n cá» khi tráº¯ng thua (tá»©c Ä‘en tháº¯ng).\n",
    "        \"\"\"\n",
    "        sampleSize = min(len(whiteWonStates), len(whiteLostStates))\n",
    "        whiteWonStates = whiteWonStates.copy()\n",
    "        whiteLostStates = whiteLostStates.copy()\n",
    "        np.random.shuffle(whiteWonStates)\n",
    "        np.random.shuffle(whiteLostStates)\n",
    "        whiteWonStates = whiteWonStates[:sampleSize]\n",
    "        whiteLostStates = whiteLostStates[:sampleSize]\n",
    "\n",
    "        # GÃ¡n label: tráº¯ng tháº¯ng â†’ 1, tráº¯ng thua (Ä‘en tháº¯ng) â†’ 0\n",
    "        whiteWonLabels = np.ones((sampleSize, 1))\n",
    "        whiteLostLabels = np.zeros((sampleSize, 1))\n",
    "\n",
    "        self.data = np.concatenate((whiteWonStates, whiteLostStates), axis=0)\n",
    "        self.labels = np.concatenate((whiteWonLabels, whiteLostLabels), axis=0)\n",
    "\n",
    "        # Shuffle cÃ¹ng lÃºc data vÃ  label\n",
    "        perm = np.random.permutation(len(self.data))\n",
    "        self.data = self.data[perm]\n",
    "        self.labels = self.labels[perm]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Tráº£ vá» tráº¡ng thÃ¡i bÃ n cá» vÃ  nhÃ£n dÆ°á»›i dáº¡ng tensor.\n",
    "\n",
    "        Returns:\n",
    "            x (Tensor): Tráº¡ng thÃ¡i bÃ n cá».\n",
    "            y (Tensor): NhÃ£n (1 náº¿u tráº¯ng tháº¯ng, 0 náº¿u tráº¯ng thua).\n",
    "        \"\"\"\n",
    "        x = torch.from_numpy(self.data[index]).float()\n",
    "        y = torch.tensor(self.labels[index]).float().squeeze()  # squeeze Ä‘á»ƒ cÃ³ dáº¡ng scalar\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3834e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1600000 white wins, 1600000 black wins\n",
      "Test: 400000 white wins, 400000 black wins\n"
     ]
    }
   ],
   "source": [
    "percentTrain = 0.8\n",
    "\n",
    "whiteWin = np.load(\"./data/whiteWin.npy\")\n",
    "whiteLost = np.load(\"./data/blackWin.npy\")\n",
    "\n",
    "whiteWinTrain = whiteWin[:int(len(whiteWin) * percentTrain)]\n",
    "whiteLostTrain = whiteLost[:int(len(whiteLost) * percentTrain)]\n",
    "whiteWinTest = whiteWin[int(len(whiteWin) * percentTrain):]\n",
    "whiteLostTest = whiteLost[int(len(whiteLost) * percentTrain):]\n",
    "\n",
    "print(f\"Train: {len(whiteWinTrain)} white wins, {len(whiteLostTrain)} black wins\")\n",
    "print(f\"Test: {len(whiteWinTest)} white wins, {len(whiteLostTest)} black wins\")\n",
    "\n",
    "dataset_train = AutoEncoderChessDataset(\n",
    "    whiteWonStates=whiteWinTrain,\n",
    "    whiteLostStates=whiteLostTrain\n",
    ")\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=512, shuffle=True)\n",
    "\n",
    "dataset_test = AutoEncoderChessDataset(\n",
    "    whiteWonStates=whiteWinTest,\n",
    "    whiteLostStates=whiteLostTest\n",
    ")\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ab5fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading AutoEncoder model from checkpoint...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for AutoEncoder:\n\tMissing key(s) in state_dict: \"encoder_layers.0.0.weight\", \"encoder_layers.0.0.bias\", \"encoder_layers.0.1.weight\", \"encoder_layers.0.1.bias\", \"encoder_layers.0.1.running_mean\", \"encoder_layers.0.1.running_var\", \"encoder_layers.1.0.weight\", \"encoder_layers.1.0.bias\", \"encoder_layers.1.1.weight\", \"encoder_layers.1.1.bias\", \"encoder_layers.1.1.running_mean\", \"encoder_layers.1.1.running_var\", \"encoder_layers.2.0.weight\", \"encoder_layers.2.0.bias\", \"encoder_layers.2.1.weight\", \"encoder_layers.2.1.bias\", \"encoder_layers.2.1.running_mean\", \"encoder_layers.2.1.running_var\", \"encoder_layers.3.0.weight\", \"encoder_layers.3.0.bias\", \"encoder_layers.3.1.weight\", \"encoder_layers.3.1.bias\", \"encoder_layers.3.1.running_mean\", \"encoder_layers.3.1.running_var\", \"decoder_layers.0.0.weight\", \"decoder_layers.0.0.bias\", \"decoder_layers.0.1.weight\", \"decoder_layers.0.1.bias\", \"decoder_layers.0.1.running_mean\", \"decoder_layers.0.1.running_var\", \"decoder_layers.1.0.weight\", \"decoder_layers.1.0.bias\", \"decoder_layers.1.1.weight\", \"decoder_layers.1.1.bias\", \"decoder_layers.1.1.running_mean\", \"decoder_layers.1.1.running_var\", \"decoder_layers.2.0.weight\", \"decoder_layers.2.0.bias\", \"decoder_layers.2.1.weight\", \"decoder_layers.2.1.bias\", \"decoder_layers.2.1.running_mean\", \"decoder_layers.2.1.running_var\", \"decoder_layers.3.0.weight\", \"decoder_layers.3.0.bias\", \"decoder_layers.3.1.weight\", \"decoder_layers.3.1.bias\", \"decoder_layers.3.1.running_mean\", \"decoder_layers.3.1.running_var\", \"encoder.0.0.weight\", \"encoder.0.0.bias\", \"encoder.0.1.weight\", \"encoder.0.1.bias\", \"encoder.0.1.running_mean\", \"encoder.0.1.running_var\", \"encoder.1.0.weight\", \"encoder.1.0.bias\", \"encoder.1.1.weight\", \"encoder.1.1.bias\", \"encoder.1.1.running_mean\", \"encoder.1.1.running_var\", \"encoder.2.0.weight\", \"encoder.2.0.bias\", \"encoder.2.1.weight\", \"encoder.2.1.bias\", \"encoder.2.1.running_mean\", \"encoder.2.1.running_var\", \"encoder.3.0.weight\", \"encoder.3.0.bias\", \"encoder.3.1.weight\", \"encoder.3.1.bias\", \"encoder.3.1.running_mean\", \"encoder.3.1.running_var\", \"decoder.0.0.weight\", \"decoder.0.0.bias\", \"decoder.0.1.weight\", \"decoder.0.1.bias\", \"decoder.0.1.running_mean\", \"decoder.0.1.running_var\", \"decoder.1.0.weight\", \"decoder.1.0.bias\", \"decoder.1.1.weight\", \"decoder.1.1.bias\", \"decoder.1.1.running_mean\", \"decoder.1.1.running_var\", \"decoder.2.0.weight\", \"decoder.2.0.bias\", \"decoder.2.1.weight\", \"decoder.2.1.bias\", \"decoder.2.1.running_mean\", \"decoder.2.1.running_var\", \"decoder.3.0.weight\", \"decoder.3.0.bias\", \"decoder.3.1.weight\", \"decoder.3.1.bias\", \"decoder.3.1.running_mean\", \"decoder.3.1.running_var\". \n\tUnexpected key(s) in state_dict: \"encoder.4.weight\", \"encoder.4.bias\", \"encoder.4.running_mean\", \"encoder.4.running_var\", \"encoder.4.num_batches_tracked\", \"encoder.6.weight\", \"encoder.6.bias\", \"encoder.7.weight\", \"encoder.7.bias\", \"encoder.7.running_mean\", \"encoder.7.running_var\", \"encoder.7.num_batches_tracked\", \"encoder.9.weight\", \"encoder.9.bias\", \"encoder.10.weight\", \"encoder.10.bias\", \"encoder.10.running_mean\", \"encoder.10.running_var\", \"encoder.10.num_batches_tracked\", \"encoder.0.weight\", \"encoder.0.bias\", \"encoder.1.weight\", \"encoder.1.bias\", \"encoder.1.running_mean\", \"encoder.1.running_var\", \"encoder.1.num_batches_tracked\", \"encoder.3.weight\", \"encoder.3.bias\", \"decoder.4.weight\", \"decoder.4.bias\", \"decoder.4.running_mean\", \"decoder.4.running_var\", \"decoder.4.num_batches_tracked\", \"decoder.6.weight\", \"decoder.6.bias\", \"decoder.7.weight\", \"decoder.7.bias\", \"decoder.7.running_mean\", \"decoder.7.running_var\", \"decoder.7.num_batches_tracked\", \"decoder.9.weight\", \"decoder.9.bias\", \"decoder.10.weight\", \"decoder.10.bias\", \"decoder.10.running_mean\", \"decoder.10.running_var\", \"decoder.10.num_batches_tracked\", \"decoder.0.weight\", \"decoder.0.bias\", \"decoder.1.weight\", \"decoder.1.bias\", \"decoder.1.running_mean\", \"decoder.1.running_var\", \"decoder.1.num_batches_tracked\", \"decoder.3.weight\", \"decoder.3.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(model_path):\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading AutoEncoder model from checkpoint...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ChessEngine/lib/python3.11/site-packages/torch/nn/modules/module.py:2581\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2573\u001b[39m         error_msgs.insert(\n\u001b[32m   2574\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2575\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2576\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2577\u001b[39m             ),\n\u001b[32m   2578\u001b[39m         )\n\u001b[32m   2580\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2581\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2582\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2583\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2584\u001b[39m         )\n\u001b[32m   2585\u001b[39m     )\n\u001b[32m   2586\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for AutoEncoder:\n\tMissing key(s) in state_dict: \"encoder_layers.0.0.weight\", \"encoder_layers.0.0.bias\", \"encoder_layers.0.1.weight\", \"encoder_layers.0.1.bias\", \"encoder_layers.0.1.running_mean\", \"encoder_layers.0.1.running_var\", \"encoder_layers.1.0.weight\", \"encoder_layers.1.0.bias\", \"encoder_layers.1.1.weight\", \"encoder_layers.1.1.bias\", \"encoder_layers.1.1.running_mean\", \"encoder_layers.1.1.running_var\", \"encoder_layers.2.0.weight\", \"encoder_layers.2.0.bias\", \"encoder_layers.2.1.weight\", \"encoder_layers.2.1.bias\", \"encoder_layers.2.1.running_mean\", \"encoder_layers.2.1.running_var\", \"encoder_layers.3.0.weight\", \"encoder_layers.3.0.bias\", \"encoder_layers.3.1.weight\", \"encoder_layers.3.1.bias\", \"encoder_layers.3.1.running_mean\", \"encoder_layers.3.1.running_var\", \"decoder_layers.0.0.weight\", \"decoder_layers.0.0.bias\", \"decoder_layers.0.1.weight\", \"decoder_layers.0.1.bias\", \"decoder_layers.0.1.running_mean\", \"decoder_layers.0.1.running_var\", \"decoder_layers.1.0.weight\", \"decoder_layers.1.0.bias\", \"decoder_layers.1.1.weight\", \"decoder_layers.1.1.bias\", \"decoder_layers.1.1.running_mean\", \"decoder_layers.1.1.running_var\", \"decoder_layers.2.0.weight\", \"decoder_layers.2.0.bias\", \"decoder_layers.2.1.weight\", \"decoder_layers.2.1.bias\", \"decoder_layers.2.1.running_mean\", \"decoder_layers.2.1.running_var\", \"decoder_layers.3.0.weight\", \"decoder_layers.3.0.bias\", \"decoder_layers.3.1.weight\", \"decoder_layers.3.1.bias\", \"decoder_layers.3.1.running_mean\", \"decoder_layers.3.1.running_var\", \"encoder.0.0.weight\", \"encoder.0.0.bias\", \"encoder.0.1.weight\", \"encoder.0.1.bias\", \"encoder.0.1.running_mean\", \"encoder.0.1.running_var\", \"encoder.1.0.weight\", \"encoder.1.0.bias\", \"encoder.1.1.weight\", \"encoder.1.1.bias\", \"encoder.1.1.running_mean\", \"encoder.1.1.running_var\", \"encoder.2.0.weight\", \"encoder.2.0.bias\", \"encoder.2.1.weight\", \"encoder.2.1.bias\", \"encoder.2.1.running_mean\", \"encoder.2.1.running_var\", \"encoder.3.0.weight\", \"encoder.3.0.bias\", \"encoder.3.1.weight\", \"encoder.3.1.bias\", \"encoder.3.1.running_mean\", \"encoder.3.1.running_var\", \"decoder.0.0.weight\", \"decoder.0.0.bias\", \"decoder.0.1.weight\", \"decoder.0.1.bias\", \"decoder.0.1.running_mean\", \"decoder.0.1.running_var\", \"decoder.1.0.weight\", \"decoder.1.0.bias\", \"decoder.1.1.weight\", \"decoder.1.1.bias\", \"decoder.1.1.running_mean\", \"decoder.1.1.running_var\", \"decoder.2.0.weight\", \"decoder.2.0.bias\", \"decoder.2.1.weight\", \"decoder.2.1.bias\", \"decoder.2.1.running_mean\", \"decoder.2.1.running_var\", \"decoder.3.0.weight\", \"decoder.3.0.bias\", \"decoder.3.1.weight\", \"decoder.3.1.bias\", \"decoder.3.1.running_mean\", \"decoder.3.1.running_var\". \n\tUnexpected key(s) in state_dict: \"encoder.4.weight\", \"encoder.4.bias\", \"encoder.4.running_mean\", \"encoder.4.running_var\", \"encoder.4.num_batches_tracked\", \"encoder.6.weight\", \"encoder.6.bias\", \"encoder.7.weight\", \"encoder.7.bias\", \"encoder.7.running_mean\", \"encoder.7.running_var\", \"encoder.7.num_batches_tracked\", \"encoder.9.weight\", \"encoder.9.bias\", \"encoder.10.weight\", \"encoder.10.bias\", \"encoder.10.running_mean\", \"encoder.10.running_var\", \"encoder.10.num_batches_tracked\", \"encoder.0.weight\", \"encoder.0.bias\", \"encoder.1.weight\", \"encoder.1.bias\", \"encoder.1.running_mean\", \"encoder.1.running_var\", \"encoder.1.num_batches_tracked\", \"encoder.3.weight\", \"encoder.3.bias\", \"decoder.4.weight\", \"decoder.4.bias\", \"decoder.4.running_mean\", \"decoder.4.running_var\", \"decoder.4.num_batches_tracked\", \"decoder.6.weight\", \"decoder.6.bias\", \"decoder.7.weight\", \"decoder.7.bias\", \"decoder.7.running_mean\", \"decoder.7.running_var\", \"decoder.7.num_batches_tracked\", \"decoder.9.weight\", \"decoder.9.bias\", \"decoder.10.weight\", \"decoder.10.bias\", \"decoder.10.running_mean\", \"decoder.10.running_var\", \"decoder.10.num_batches_tracked\", \"decoder.0.weight\", \"decoder.0.bias\", \"decoder.1.weight\", \"decoder.1.bias\", \"decoder.1.running_mean\", \"decoder.1.running_var\", \"decoder.1.num_batches_tracked\", \"decoder.3.weight\", \"decoder.3.bias\". "
     ]
    }
   ],
   "source": [
    "# model = AutoEncoder(\n",
    "#     layer=[773, 600, 400, 200, 100]\n",
    "# )\n",
    "# model_path = \"./checkpoints/AutoEncoder.pth\"\n",
    "# if os.path.exists(model_path):\n",
    "#     print(\"Loading AutoEncoder model from checkpoint...\")\n",
    "#     model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63863b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x):\n",
    "    return F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "\n",
    "def mse_loss_function(recon_x, x):\n",
    "    return F.mse_loss(recon_x, x, reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cd7289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, dataloader, optimizer, mse_loss_function, device, model_path):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "    \n",
    "#     for i, (x, _) in enumerate(tqdm(dataloader)):\n",
    "#         x = x.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         recon_batch, _ = model(x)\n",
    "#         loss = mse_loss_function(recon_batch, x)\n",
    "\n",
    "#         # Backward pass and optimization\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#     train_loss = running_loss / len(dataloader)\n",
    "#     return train_loss\n",
    "\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def test(model, dataloader, mse_loss_function, device):\n",
    "#     model.eval()\n",
    "#     total_loss = 0.0\n",
    "\n",
    "#     for x, _ in dataloader:\n",
    "#         x = x.to(device)\n",
    "\n",
    "#         recon_batch, _ = model(x)\n",
    "#         loss = mse_loss_function(recon_batch, x)\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     avg_loss = total_loss / len(dataloader)\n",
    "#     print(f\"Test Loss: {avg_loss:.6f}\")\n",
    "#     return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a1c748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 17637/25000 [02:31<00:59, 124.66it/s]"
     ]
    }
   ],
   "source": [
    "# epochs = 1000\n",
    "# patience = 5\n",
    "# best_loss = float('inf')\n",
    "# patience = 5\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     train_loss = train(model, dataloader_train, optimizer, mse_loss_function, device, model_path)\n",
    "#     test_loss = test(model, dataloader_test, mse_loss_function, device)\n",
    "    \n",
    "#     print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.6f}, Test Loss: {test_loss:.6f}\")\n",
    "    \n",
    "#     if test_loss < best_loss:\n",
    "#         best_loss = test_loss\n",
    "#         patience_counter = 0\n",
    "#         torch.save(model.state_dict(), model_path)\n",
    "#         print(\"âœ… Saved best model\")\n",
    "#     else:\n",
    "#         patience_counter += 1\n",
    "#         print(f\"â¸ï¸ No improvement. Patience counter: {patience_counter}/{patience}\")\n",
    "#         if patience_counter >= patience:\n",
    "#             print(\"ðŸ›‘ Early stopping triggered\")\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31466113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining layer 0: 773 â†’ 600\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     38\u001b[39m             current_input = model.encoder_layers[i](current_input)\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pretrained_weights\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[43mlayerwise_pretrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mlayerwise_pretrain\u001b[39m\u001b[34m(model, epochs, batch_size, lr)\u001b[39m\n\u001b[32m     26\u001b[39m recon = decoder(z)\n\u001b[32m     27\u001b[39m loss = criterion(recon, x_batch)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m optimizer.step()\n\u001b[32m     30\u001b[39m total_loss += loss.item() * x_batch.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ChessEngine/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ChessEngine/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ChessEngine/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = AutoEncoder(\n",
    "    layer =[773, 600, 400, 200, 100]\n",
    ")\n",
    "def layerwise_pretrain(model, epochs=30, lr=1e-3, patience=5):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    pretrained_weights = []\n",
    "\n",
    "    for i in range(len(model.layer) - 1):\n",
    "        print(f\"Pretraining layer {i}: {model.layer[i]} â†’ {model.layer[i+1]}\")\n",
    "\n",
    "        # Shallow Autoencoder\n",
    "        encoder = model.encoder_layers[i][0]  # Linear layer\n",
    "        decoder = nn.Linear(model.layer[i + 1], model.layer[i]).to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=lr)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for x_batch, _ in dataloader_train:\n",
    "                x_batch = x_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                z = model.encoder_layers[i](x_batch)\n",
    "                recon = decoder(z)\n",
    "                loss = criterion(recon, x_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item() * x_batch.size(0)\n",
    "            epoch_loss = total_loss / len(dataloader_train.dataset)\n",
    "\n",
    "            if epoch_loss < best_loss - 1e-4:  # Some delta\n",
    "                best_loss = epoch_loss\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(f\"  Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "        # Save decoder for optional reconstruction later\n",
    "        pretrained_weights.append((encoder.weight.data.clone(), encoder.bias.data.clone()))\n",
    "\n",
    "        # Update current input to next layer\n",
    "        with torch.no_grad():\n",
    "            current_input = model.encoder_layers[i](current_input)\n",
    "\n",
    "    return pretrained_weights\n",
    "\n",
    "layerwise_pretrain(model, epochs=30, batch_size=128, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200fd7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def layerwise_pretrain(model, trainX, epochs=30, lr=1e-3, patience=5, batch_size=128):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Initialize dataloader\n",
    "    dataset = AutoEncoderChessDataset(trainX[0], trainX[1])\n",
    "    dataloader_train = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    pretrained_weights = []\n",
    "\n",
    "    for i in range(len(model.layer) - 1):\n",
    "        print(f\"Pretraining layer {i}: {model.layer[i]} â†’ {model.layer[i+1]}\")\n",
    "\n",
    "        # Initialize best_loss and epochs_no_improve for early stopping\n",
    "        best_loss = float('inf')\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        # Shallow Autoencoder\n",
    "        encoder = model.encoder_layers[i][0]  # nn.Linear\n",
    "        decoder = nn.Linear(model.layer[i + 1], model.layer[i]).to(device)\n",
    "\n",
    "        optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=lr)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for x_batch, _ in dataloader_train:\n",
    "                x_batch = x_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                z = model.encoder_layers[i](x_batch)\n",
    "                recon = decoder(z)\n",
    "                loss = criterion(recon, x_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item() * x_batch.size(0)\n",
    "\n",
    "            epoch_loss = total_loss / len(dataloader_train.dataset)\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.6f}\")\n",
    "\n",
    "            # Early stopping logic\n",
    "            if epoch_loss < best_loss - 1e-4:  # Some delta\n",
    "                best_loss = epoch_loss\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(f\"  Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "\n",
    "        # Save decoder weights\n",
    "        pretrained_weights.append((encoder.weight.data.clone(), encoder.bias.data.clone()))\n",
    "\n",
    "        # Feedforward output for next layer\n",
    "        with torch.no_grad():\n",
    "            current_input = model.encoder_layers[i](current_input)\n",
    "\n",
    "    return pretrained_weights\n",
    "\n",
    "# Example usage\n",
    "percentTrain = 0.8\n",
    "\n",
    "whiteWin = np.load(\"./data/whiteWin.npy\")\n",
    "whiteLost = np.load(\"./data/blackWin.npy\")\n",
    "\n",
    "whiteWinTrain = whiteWin[:int(len(whiteWin) * percentTrain)]\n",
    "whiteLostTrain = whiteLost[:int(len(whiteLost) * percentTrain)]\n",
    "whiteWinTest = whiteWin[int(len(whiteWin) * percentTrain):]\n",
    "whiteLostTest = whiteLost[int(len(whiteLost) * percentTrain):]\n",
    "\n",
    "print(f\"Train: {len(whiteWinTrain)} white wins, {len(whiteLostTrain)} black wins\")\n",
    "print(f\"Test: {len(whiteWinTest)} white wins, {len(whiteLostTest)} black wins\")\n",
    "\n",
    "# Prepare training data\n",
    "dataset_train = AutoEncoderChessDataset(\n",
    "    whiteWonStates=whiteWinTrain,\n",
    "    whiteLostStates=whiteLostTrain\n",
    ")\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=512, shuffle=True)\n",
    "\n",
    "# Pretrain with early stopping\n",
    "model = AutoEncoder(layer=[773, 600, 400, 200, 100])\n",
    "layerwise_pretrain(model, (whiteWinTrain, whiteLostTrain), epochs=30, batch_size=128, lr=1e-2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ChessEngine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
